[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TORCHLITE Hackathon Handbook",
    "section": "",
    "text": "About TORCHLITE\n\nTools for Open Research and Computation with HathiTrust: Leveraging Intelligent Text Extraction (TORCHLITE)\n\nIt is a next-generation Web-based, interactive visualization and analytical tool dashboard that consumes existing data from our one-of-a-kind, fully open Extracted Features (EF) dataset, along with a well-documented API to allow our user community to develop its own tools for interacting with data from the 17.5-million-volume HathiTrust Digital Library.\nThrough this hackathon, we aim to develop and promote these tools and the API. The EF dataset contains nearly 3 trillion tokens representing over 6 billion pages, making it arguably the largest open dataset of its kind readily available to digital humanities and other scholars in the world."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Welcome to “Introduction to Topic Modeling & Topic Visualization for Social Scientists”!\nTopic modeling is a powerful technique that allows social scientists to analyze large volumes of text data and uncover hidden patterns and themes. In recent years, topic modeling has become increasingly popular in the social sciences, with researchers using it to analyze everything from social media data to political speeches. However, many social scientists may be unsure of how to get started with topic modeling, or may not fully understand the potential of its various algorithms built.\n\nThis book aims to fill that gap, providing a practical guide to topic modeling for social scientists. It will cover the basics of topic modeling, including its underlying concepts, as well as more advanced techniques such as comparison of different algorithms, interpretability, evaluation, and visualization. Further, due to the presence of multiple topic modeling algorithms, they might be confused as to which one to use for their research. This book will help them to solve this problem by providing hands-on examples and case studies drawn from real-world applications in the social sciences, showing readers how topic modeling can be used to answer a wide range of research questions.\n\nThe book will be divided into three main sections. In the first section, we will introduce the basics of topic modeling, including its underlying concepts and algorithms. We will also provide an overview of the different types of topic models available, and discuss the pros and cons of each. In the second section, we will provide a step-by-step guide (both R and Python) to conducting a topic modeling analysis, including data preparation, model fitting, and interpretation of results. We will also cover advanced techniques such as model comparison, interpretability, evaluation, visualization and provide practical tips for troubleshooting common problems. In the third section, we will present a series of case studies drawn from real-world applications in the social sciences. These case studies will showcase the diversity of ways in which topic modeling can be used in research, and provide readers with a sense of the potential of this technique.\nFurthermore, the author intends to enrich the book by adding a \"story\" section where she will conduct interviews with social science researchers who have utilized topic modeling in their research. This approach aims to provide readers with practical and real-world examples of how topic modeling can be applied in various social science fields.\nIn summary, this book will provide social scientists with the knowledge and skills they need to confidently use topic modeling in their own research. By the end of the book, readers will have a solid understanding of the basics of topic modeling, as well as the ability to conduct their own analyses and interpret the results.\n\nThis book is intended for graduate students and researchers in the social sciences who are interested in learning about topic modeling and its potential applications in their research. It will be useful for those with a background in statistics and data analysis, as well as those with little or no prior experience with topic modeling."
  },
  {
    "objectID": "chap-1.html#what-is-topic-modeling",
    "href": "chap-1.html#what-is-topic-modeling",
    "title": "1  Topic Modeling and Its Applications?",
    "section": "1.1 What is Topic Modeling?",
    "text": "1.1 What is Topic Modeling?\nTopic modeling is a popular computer-assisted technique that assist social scientists to find patterns in a haystack of documents. It helps in discovering knowledge and uncovering evidence already present in the corpus."
  },
  {
    "objectID": "chap-1.html#history-of-topic-modeling",
    "href": "chap-1.html#history-of-topic-modeling",
    "title": "1  Topic Modeling and Its Applications?",
    "section": "1.2 History of topic modeling",
    "text": "1.2 History of topic modeling"
  },
  {
    "objectID": "chap-1.html#why-are-topic-models-useful",
    "href": "chap-1.html#why-are-topic-models-useful",
    "title": "1  Topic Modeling and Its Applications?",
    "section": "1.3 Why are topic models useful?",
    "text": "1.3 Why are topic models useful?"
  },
  {
    "objectID": "chap-1.html#tools-used-for-topic-modeling",
    "href": "chap-1.html#tools-used-for-topic-modeling",
    "title": "1  Topic Modeling and Its Applications?",
    "section": "1.4 Tools Used for topic modeling",
    "text": "1.4 Tools Used for topic modeling"
  },
  {
    "objectID": "chap-1.html#applications-of-topic-modeling",
    "href": "chap-1.html#applications-of-topic-modeling",
    "title": "1  Topic Modeling and Its Applications?",
    "section": "1.5 Applications of topic modeling",
    "text": "1.5 Applications of topic modeling\n\n1.5.1 Information Retrieval\n\n\n1.5.2 Document Language Models\n\n\n1.5.3 Query Expansion\n\n\n1.5.4 Search Personalization\n\n\n1.5.5 Historical Documents\n\n\n1.5.6 Understanding Scientific Publications\n\n\n1.5.7 Fiction & Literature\n\n\n1.5.8 Computational Social Science\n\n\n1.5.9 Multilingual Data & Machine Translation"
  },
  {
    "objectID": "chap-1.html#additional-resources",
    "href": "chap-1.html#additional-resources",
    "title": "1  Topic Modeling and Its Applications?",
    "section": "1.6 Additional Resources",
    "text": "1.6 Additional Resources\n\nhttps://mimno.infosci.cornell.edu/topics.html"
  },
  {
    "objectID": "chap-2.html#social-science-research-data",
    "href": "chap-2.html#social-science-research-data",
    "title": "2  Text Pre-Processing",
    "section": "2.1 Social Science Research Data",
    "text": "2.1 Social Science Research Data\nIn the realm of social science research, text data encompasses a broad spectrum of sources. These may include survey responses, interview transcripts, legal documents, government reports, archival records, blog posts, emails, instant messaging chats, reviews, letters, literary texts such as novels and poems, advertising materials, media content, speech transcripts, research papers, online forum discussions, Reddit threads, and social media content from platforms like Twitter, Facebook, Instagram, and YouTube, as well as newspaper articles, among others.\nFurther, in social science research, the primary focus lies in deciphering the underlying social, political, and economic dynamics embedded within the text rather than merely analyzing the text itself. This nuanced approach allows researchers to glean valuable insights into the intricacies of human behavior and societal phenomena.\nGrimmer et al. (2022) outline four distinct approaches to corpus construction:\n\nQuestion-Specific Corpus Construction: The effectiveness of a corpus depends on the research question at hand and the population they intend to investigate. For instance, product reviews, social media discussions, and market analysis reports related to plant-based food products in the European market can be used to capture the evolving consumer sentiments and preferences towards these products.\nNo-Values Free Corpus Construction: In the process of corpus construction, it is imperative to prioritize ethical considerations regarding representation and privacy. Researchers may inadvertently neglect to represent a diverse population, potentially resulting in skewed conclusions and inaccuracies."
  },
  {
    "objectID": "chap-2.html#cleaning-and-pre-processing",
    "href": "chap-2.html#cleaning-and-pre-processing",
    "title": "2  Text Pre-Processing",
    "section": "2.2 Cleaning and Pre-Processing",
    "text": "2.2 Cleaning and Pre-Processing\nFor text data, it’s crucial to distinguish the information that holds significance for the researcher from that which does not. In this context, we will explore various methods for text representation, including bags of words, word vectors, and more."
  },
  {
    "objectID": "chap-2.html#section",
    "href": "chap-2.html#section",
    "title": "2  Text Pre-Processing",
    "section": "2.3 ",
    "text": "2.3"
  },
  {
    "objectID": "chap-5.html#text-document-selection-and-biases",
    "href": "chap-5.html#text-document-selection-and-biases",
    "title": "3  Topic Modeling and Social Science Research",
    "section": "3.1 Text Document Selection and Biases",
    "text": "3.1 Text Document Selection and Biases\nIn any text analysis process, including topic modeling, the initial step involves exploring specific collections of documents. Research is inherently a non-linear process, often requiring multiple iterations of corpus construction as researchers strive to pinpoint intriguing questions for their research problem, as pointed out by Grimmer et al. (2022). In most social science research, scholars typically commence by formulating a well-defined research question and subsequently selecting an appropriate corpus. However, there are instances where researchers’ questions evolve over time, leading to corresponding changes in their chosen corpus. This adaptive approach is necessitated by the challenge of understanding how the data aligns with their research questions without conducting initial analyses. In both scenarios, the process of document selection can introduce four potential selection biases, particularly in the context of social science text analysis research, as elucidated by Grimmer et al. (2022):\n\nResource Bias: It refers to the use of a corpus that is readily available for analysis. Data is the new gold1. Thus, in our case, getting good quality text data is expensive as texts (particularly historical and government texts) can be burned, lost, or not stored/transcribed/digitized. For instance,\nIncentive Bias: It is the retention or production of text documents resulting from strategic behavior such as individuals/organizations having incentives to delete/hide/destroy text documents that can put them in a bad light or people posting only positive aspects of their lives on social media."
  },
  {
    "objectID": "chap-5.html#footnotes",
    "href": "chap-5.html#footnotes",
    "title": "3  Topic Modeling and Social Science Research",
    "section": "",
    "text": "https://www.forbes.com/sites/forbestechcouncil/2023/03/27/how-to-make-use-of-the-new-gold-data/?sh=2855954a2bbf↩︎"
  },
  {
    "objectID": "preface.html#hathitrust-research-center",
    "href": "preface.html#hathitrust-research-center",
    "title": "About HathiTrust and HTRC",
    "section": "HathiTrust Research Center",
    "text": "HathiTrust Research Center\nThe HathiTrust Research Center (HTRC) enables large-scale analysis of works in the Hathi Trust Digital Library (HTDL) to facilitate the collection’s non-profit research and educational uses. HTRC co-located at Indiana University and the University of Illinois at Urbana-Champaign, conducts research and development for text analysis of massive digital libraries. The Center creates and maintains a suite of tools and services for text-based, data-driven research and engages in cutting-edge research on large-scale data analysis. HTRC makes available the collection for text and data mining purposes while remaining clearly within the bounds of the fair use rights courts have recognized as applying to text analysis."
  },
  {
    "objectID": "preface.html#relationship-to-hathitrust",
    "href": "preface.html#relationship-to-hathitrust",
    "title": "About HathiTrust and HTRC",
    "section": "Relationship to HathiTrust",
    "text": "Relationship to HathiTrust\nHathiTrust is a partnership of academic and research institutions, offering a collection of millions of titles digitized from libraries around the world. Individual access and preservation are important concerns for Hathi Trust. It allows users to search for and build collections of digitized works, and to read those in the public domain. The Research Center focuses on the aggregate strengths: what can we learn from so many books? Digitization has enabled large-scale questions that we couldn’t ask before, and the Research Center is here to help you ask them while working within the restrictions of intellectual property law.\n\nHTRC Analytics"
  },
  {
    "objectID": "index.html#hathitrust-research-center",
    "href": "index.html#hathitrust-research-center",
    "title": "TORCHLITE Hackathon Handbook",
    "section": "HathiTrust Research Center",
    "text": "HathiTrust Research Center\nThe HathiTrust Research Center (HTRC) enables large-scale analysis of works in the Hathi Trust Digital Library (HTDL) to facilitate the collection’s non-profit research and educational uses. HTRC co-located at Indiana University and the University of Illinois at Urbana-Champaign, conducts research and development for text analysis of massive digital libraries. The Center creates and maintains a suite of tools and services for text-based, data-driven research and engages in cutting-edge research on large-scale data analysis. HTRC makes available the collection for text and data mining purposes while remaining clearly within the bounds of the fair use rights courts have recognized as applying to text analysis."
  },
  {
    "objectID": "index.html#relationship-to-hathitrust",
    "href": "index.html#relationship-to-hathitrust",
    "title": "TORCHLITE Hackathon Handbook",
    "section": "Relationship to HathiTrust",
    "text": "Relationship to HathiTrust\nHathiTrust is a partnership of academic and research institutions, offering a collection of millions of titles digitized from libraries around the world. Individual access and preservation are important concerns for Hathi Trust. It allows users to search for and build collections of digitized works, and to read those in the public domain. The Research Center focuses on the aggregate strengths: what can we learn from so many books? Digitization has enabled large-scale questions that we couldn’t ask before, and the Research Center is here to help you ask them while working within the restrictions of intellectual property law.\n\nHTRC Analytics"
  },
  {
    "objectID": "index.html#about-hathitrust-and-htrc",
    "href": "index.html#about-hathitrust-and-htrc",
    "title": "TORCHLITE Hackathon Handbook",
    "section": "About HathiTrust and HTRC",
    "text": "About HathiTrust and HTRC"
  },
  {
    "objectID": "chap-1.html#about-mantis",
    "href": "chap-1.html#about-mantis",
    "title": "1  Technical Background",
    "section": "1.1 About MANTIS",
    "text": "1.1 About MANTIS\nMantis is a react admin template having aesthetics derived from ant design and components used from MUI. Mantis is a developer-friendly & highly customizable React Admin Template based on MUI. Mantis has plenty of ready-to-use M-UI components that will help you build your site faster and save your development time. We’ve followed the best industry standards to make our product easy, fast & highly scalable to work with. It’s made with this high-end technology stack - React Hooks, Components, Create React App & Redux."
  },
  {
    "objectID": "chap-1.html#technology-stack",
    "href": "chap-1.html#technology-stack",
    "title": "1  Technical Background",
    "section": "1.2 Technology Stack",
    "text": "1.2 Technology Stack\n\nMaterial UI v5 components library\nAuthentication Methods - Auth0, Firebase, JWT, AWS\nRedux toolkit\nReact Hooks API\nRedux & React Context API for State Management\nReact Router\nAxios\nCreate React App\nCode Splitting\nCSS-in-JS\nMulti-Language"
  },
  {
    "objectID": "chap-1.html#torchlight-next-js-backend-api",
    "href": "chap-1.html#torchlight-next-js-backend-api",
    "title": "1  Technical Background",
    "section": "1.3 Torchlight Next JS Backend API",
    "text": "1.3 Torchlight Next JS Backend API\nThis document describes the models and API specification for the Torchlite NextJS backend, which supports the needs of the NextJS front-end interface. The Torchlite application is composed of the following components:\n\nthe NextJS-based front-end interface (what the user sees in their browser)\nthe NextJS backend API (the server-side piece of the NextJS application)\nthe Torchlite API (implements the Torchlite business logic for dashboards, filters, and data management)\nthe Extracted Features API (used for accessing Extracted Features data for volumes and worksets)\nthe Registry API (used for retrieving public and user-private worksets managed through Analytics Gateway)\n\nFrom the point of view of the Torchlite NextJS-based front-end interface, only the API exposed by the NextJS backend is relevant; there should be no direct communication between the front end and any of the other components outlined above.\nThe following sections will focus on the relationship between the front-end and NextJS backend, providing details about how they should function together."
  },
  {
    "objectID": "chap-2.html#htrc-extracted-features-dataset",
    "href": "chap-2.html#htrc-extracted-features-dataset",
    "title": "2  HTRC Extracted Features",
    "section": "2.1 HTRC Extracted Features Dataset",
    "text": "2.1 HTRC Extracted Features Dataset\n\nPublic domain, downloadable\nStructured data consisting of human-supplied (catalog) metadata and algorithmically-derived features\nFrom 17.1 million volumes (i.e., not quite in sync with HTDL)\nLinked-data compliant (JSON-LD)"
  },
  {
    "objectID": "chap-2.html#htrc-extracted-features-ef",
    "href": "chap-2.html#htrc-extracted-features-ef",
    "title": "2  HTRC Extracted Features",
    "section": "2.2 HTRC Extracted Features (EF)",
    "text": "2.2 HTRC Extracted Features (EF)\nThe features are:\n\nVolume- and page-level\nSelected data and metadata\nExtracted from raw text\n\nPosition the researcher to begin analysis\n\nSome standard natural language & statistical preprocessing is already done"
  },
  {
    "objectID": "chap-2.html#per-volume-features",
    "href": "chap-2.html#per-volume-features",
    "title": "2  HTRC Extracted Features",
    "section": "2.3 Per-volume features",
    "text": "2.3 Per-volume features\nExcerpted from catalog metadata, including:\n\nTitle\nAuthor\nLanguage\nPublication data\nIdentifiers\n[Subjects]"
  },
  {
    "objectID": "htrc.html#hathitrust-research-center",
    "href": "htrc.html#hathitrust-research-center",
    "title": "About HathiTrust and HTRC",
    "section": "HathiTrust Research Center",
    "text": "HathiTrust Research Center\nThe HathiTrust Research Center (HTRC) enables large-scale analysis of works in the Hathi Trust Digital Library (HTDL) to facilitate the collection’s non-profit research and educational uses. HTRC co-located at Indiana University and the University of Illinois at Urbana-Champaign, conducts research and development for text analysis of massive digital libraries. The Center creates and maintains a suite of tools and services for text-based, data-driven research and engages in cutting-edge research on large-scale data analysis. HTRC makes available the collection for text and data mining purposes while remaining clearly within the bounds of the fair use rights courts have recognized as applying to text analysis."
  },
  {
    "objectID": "htrc.html#relationship-to-hathitrust",
    "href": "htrc.html#relationship-to-hathitrust",
    "title": "About HathiTrust and HTRC",
    "section": "Relationship to HathiTrust",
    "text": "Relationship to HathiTrust\nHathiTrust is a partnership of academic and research institutions, offering a collection of millions of titles digitized from libraries around the world. Individual access and preservation are important concerns for Hathi Trust. It allows users to search for and build collections of digitized works, and to read those in the public domain. The Research Center focuses on the aggregate strengths: what can we learn from so many books? Digitization has enabled large-scale questions that we couldn’t ask before, and the Research Center is here to help you ask them while working within the restrictions of intellectual property law.\n\nHTRC Analytics"
  },
  {
    "objectID": "ef.html#htrc-extracted-features-dataset",
    "href": "ef.html#htrc-extracted-features-dataset",
    "title": "2  Extracted Features",
    "section": "2.1 HTRC Extracted Features Dataset",
    "text": "2.1 HTRC Extracted Features Dataset\n\nPublic domain, downloadable\nStructured data consisting of human-supplied (catalog) metadata and algorithmically-derived features\nFrom 17.1 million volumes (i.e., not quite in sync with HTDL)\nLinked-data compliant (JSON-LD)"
  },
  {
    "objectID": "ef.html#htrc-extracted-features-ef",
    "href": "ef.html#htrc-extracted-features-ef",
    "title": "2  Extracted Features",
    "section": "2.2 HTRC Extracted Features (EF)",
    "text": "2.2 HTRC Extracted Features (EF)\nThe features are:\n\nVolume- and page-level\nSelected data and metadata\nExtracted from raw text\n\nPosition the researcher to begin analysis\n\nSome standard natural language & statistical preprocessing is already done"
  },
  {
    "objectID": "ef.html#per-volume-features",
    "href": "ef.html#per-volume-features",
    "title": "2  Extracted Features",
    "section": "2.3 Per-volume features",
    "text": "2.3 Per-volume features\nExcerpted from catalog metadata, including:\n\nTitle\nAuthor\nLanguage\nPublication data\nIdentifiers\n[Subjects]\n\n\n\nHTRC Extracted Features API documentation is available here"
  },
  {
    "objectID": "technical-background.html#about-mantis",
    "href": "technical-background.html#about-mantis",
    "title": "1  Technical Background",
    "section": "1.1 About MANTIS",
    "text": "1.1 About MANTIS\nMantis is a react admin template having aesthetics derived from ant design and components used from MUI. Mantis is a developer-friendly & highly customizable React Admin Template based on MUI. Mantis has plenty of ready-to-use M-UI components that will help you build your site faster and save your development time. We’ve followed the best industry standards to make our product easy, fast & highly scalable to work with. It’s made with this high-end technology stack - React Hooks, Components, Create React App & Redux."
  },
  {
    "objectID": "technical-background.html#technology-stack",
    "href": "technical-background.html#technology-stack",
    "title": "1  Technical Background",
    "section": "1.2 Technology Stack",
    "text": "1.2 Technology Stack\n\nMaterial UI v5 components library\nAuthentication Methods - Auth0, Firebase, JWT, AWS\nRedux toolkit\nReact Hooks API\nRedux & React Context API for State Management\nReact Router\nAxios\nCreate React App\nCode Splitting\nCSS-in-JS\nMulti-Language"
  },
  {
    "objectID": "technical-background.html#torchlight-next-js-backend-api",
    "href": "technical-background.html#torchlight-next-js-backend-api",
    "title": "1  Technical Background",
    "section": "1.3 Torchlight Next JS Backend API",
    "text": "1.3 Torchlight Next JS Backend API\nThis document describes the models and API specification for the Torchlite NextJS backend, which supports the needs of the NextJS front-end interface. The Torchlite application is composed of the following components:\n\nthe NextJS-based front-end interface (what the user sees in their browser)\nthe NextJS backend API (the server-side piece of the NextJS application)\nthe Torchlite API (implements the Torchlite business logic for dashboards, filters, and data management)\nthe Extracted Features API (used for accessing Extracted Features data for volumes and worksets)\nthe Registry API (used for retrieving public and user-private worksets managed through Analytics Gateway)\n\nFrom the point of view of the Torchlite NextJS-based front-end interface, only the API exposed by the NextJS backend is relevant; there should be no direct communication between the front end and any of the other components outlined above.\nThe following sections will focus on the relationship between the front-end and NextJS backend, providing details about how they should function together."
  },
  {
    "objectID": "ef.html#api-calls",
    "href": "ef.html#api-calls",
    "title": "2  Extracted Features",
    "section": "2.4 API CALLS",
    "text": "2.4 API CALLS\n\nGET EF data for a volume by volume id\n\nhttps://tools.htrc.illinois.edu/ef-api/volumes/{clean-htid}\n\nCheck if a volume exists (HEAD)\n\nhttps://tools.htrc.illinois.edu/ef-api/volumes/{clean-htid}\n\nGET volume metadata by volume id\n\nhttps://tools.htrc.illinois.edu/ef-api/volumes/{clean-htid}/metadata\n\nGET subset of pages of volume by volume id\n\nhttps://tools.htrc.illinois.edu/ef-api/volumes/{clean-htid}/pages\n\nCreate workset (POST)\n\nhttps://tools.htrc.illinois.edu/ef-api/worksets\n\nDELETE workset by workset id\n\nhttps://tools.htrc.illinois.edu/ef-api/worksets/{workset-id}\n\nGET workset\n\nhttps://tools.htrc.illinois.edu/ef-api/worksets/{workset-id}\n\nGET workset volumes by workset id\n\nhttps://tools.htrc.illinois.edu/ef-api/worksets/{workset-id}/volumes\n\nGET workset volumes metadata by workset id o\n\nhttps://tools.htrc.illinois.edu/ef-api/worksets/{workset-id}/metadata"
  },
  {
    "objectID": "ef.html#observable-notebooks",
    "href": "ef.html#observable-notebooks",
    "title": "2  Extracted Features",
    "section": "2.5 OBSERVABLE Notebooks",
    "text": "2.5 OBSERVABLE Notebooks\nOBSERVABLE Documentation\nOBSERVABLE Documentation (Shorter)\n\nExploring API\nWord Cloud\nContributor Map"
  },
  {
    "objectID": "worksets.html#section",
    "href": "worksets.html#section",
    "title": "3  Featured Worksets",
    "section": "3.1 ",
    "text": "3.1"
  },
  {
    "objectID": "worksets.html#ready-made-featured-worksets",
    "href": "worksets.html#ready-made-featured-worksets",
    "title": "3  Featured Worksets",
    "section": "3.1 Ready-Made Featured Worksets",
    "text": "3.1 Ready-Made Featured Worksets\n\n\n\n\n\n\n\n\n\nName\nWorkset ID\n# of Volumes\nDescription\n\n\nNative-authored Workset\n6408a5e930000067055ce2e8\n919\nA list of works in HathiTrust authored by Native Americans, identified by Drs. Raymond Orr, Raina Heaton, Kun Lu\n\n\nComparative Literature Workset\n6408a63a30000067055ce2e9\n927\nA list of works in HathiTrust authored by non-Native Americans with subjects, language representation, and publicationyear similar to those in the Native- authored workset\n\n\nNative-authored Workset Sample\n640e74be300000d90b5ce31b\n10\n10 volume sample from the list of works authored by Native Americans in HathiTrust\n\n\nThe Black Fantastic Workset\n642720e53300008d02a5dc9c\n90\n90 volumes, including duplicates, of Black Fantastic fiction, identified by Drs.Seretha Williams and Clarissa West- White\n\n\nHistory of Black Writers Workset\n6407a1ec300000a4045ce2e7\n2166\nVolumes found in HathiTrust written by African-American authors, as identified by the Dr. Maryemma Graham and colleagues in the University of Kansas’s Project on the History of Black Writing\n\n\nHistory of Black Writers Workset Sample\n640ba4bf300000440a5ce31a\n25\n25 sample volumes from “History of Black Writers” Workset"
  },
  {
    "objectID": "worksets.html#individual-volumes-to-explore",
    "href": "worksets.html#individual-volumes-to-explore",
    "title": "3  Featured Worksets",
    "section": "3.2 Individual Volumes to Explore",
    "text": "3.2 Individual Volumes to Explore\n\nuva.x001016358, The children of Sisyphus by Orlando Patterson\nmdp.39015016452230, Trumbull park, a novel by Frank London Brown"
  },
  {
    "objectID": "widget.html#overview",
    "href": "widget.html#overview",
    "title": "4  Creating Summary Widget and Notebook",
    "section": "4.1 Overview",
    "text": "4.1 Overview"
  },
  {
    "objectID": "widget.html#exploring-textual-data-with-the-torchlite-summary-widget",
    "href": "widget.html#exploring-textual-data-with-the-torchlite-summary-widget",
    "title": "4  Creating Summary Widget and Notebook",
    "section": "4.2 Exploring Textual Data with the Torchlite Summary Widget",
    "text": "4.2 Exploring Textual Data with the Torchlite Summary Widget\nIn this Hackathon, we’re excited to introduce participants to the “Torchlite Summary” widget, a powerful tool designed to enhance your experience with textual data analysis.\nDeveloped using Python in a Jupyter Notebook environment and leveraging the HathiTrust Research Center (HTRC) APIs, this widget offers a streamlined approach to summarizing and exploring large datasets. Whether you’re analyzing literary works, historical documents, or any extensive text corpus, “Torchlite Summary” provides key insights through word frequencies, trends, and thematic overviews.\nWe encourage all participants to utilize this tool to uncover hidden patterns, compare textual features, and drive their projects to new depths of analysis. A step-by-step guide within the notebook will help you get started, offering insights into API integration, data visualization, and customization options to tailor the tool to your specific research questions.\n\nCode Base of Torch lite Summary\nHathiTrust-API"
  },
  {
    "objectID": "publications.html#overview",
    "href": "publications.html#overview",
    "title": "5  Relevant Publications",
    "section": "5.1 Overview",
    "text": "5.1 Overview"
  },
  {
    "objectID": "publications.html#exploring-textual-data-with-the-torchlite-summary-widget",
    "href": "publications.html#exploring-textual-data-with-the-torchlite-summary-widget",
    "title": "5  Relevant Publications",
    "section": "5.2 Exploring Textual Data with the Torchlite Summary Widget",
    "text": "5.2 Exploring Textual Data with the Torchlite Summary Widget\nIn this Hackathon, we’re excited to introduce participants to the “Torchlite Summary” widget, a powerful tool designed to enhance your experience with textual data analysis.\nDeveloped using Python in a Jupyter Notebook environment and leveraging the HathiTrust Research Center (HTRC) APIs, this widget offers a streamlined approach to summarizing and exploring large datasets. Whether you’re analyzing literary works, historical documents, or any extensive text corpus, “Torchlite Summary” provides key insights through word frequencies, trends, and thematic overviews.\nWe encourage all participants to utilize this tool to uncover hidden patterns, compare textual features, and drive their projects to new depths of analysis. A step-by-step guide within the notebook will help you get started, offering insights into API integration, data visualization, and customization options to tailor the tool to your specific research questions.\n\nCode Base of Torch lite Summary\nHathiTrust-API"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "5  Relevant Publications",
    "section": "",
    "text": "Walsh, J.A., Layne-Worthey, G., Jett, J., Capitanu, B., Organisciak, P., Dubnicek, R., and Downie, J.S. (2023). The Library is Open!: Open Data and an Open API for the HathiTrust Digital Library. In CHR2023: Computational Humanities Research Conference. https://ceur-ws.org/Vol-3558/paper7875.pdf"
  },
  {
    "objectID": "widget.html#different-visualizations",
    "href": "widget.html#different-visualizations",
    "title": "4  Creating Summary Widget and Notebook",
    "section": "4.3 Different Visualizations",
    "text": "4.3 Different Visualizations\nIn this section, we will summarize some of the sample visualizations in different programming languages that can be made using TORCHLITE API.\n\n4.3.1 Word Cloud\nA word cloud is a visualization technique for displaying text data in which the importance of each word is shown through its size. Typically, the more frequently a word appears in the text, the larger it appears in the word cloud. It’s useful for quickly identifying the most prominent terms in a dataset or text corpus.\nThe Word Cloud widget is being displayed using React and D3 libraries. Here’s a high-level overview of how it works:\n\n4.3.1.1 Internal working of Word Cloud:\n\nData Fetching: The component fetches data from an external API endpoint (https://tools.htrc.illinois.edu/ef-api/worksets/${worksetId}/volumes) using fetch API. This API returns volumes of text data along with their token counts.\nData Processing: Once the data is fetched, it is processed to extract the token counts for each word. Common words and punctuation marks are filtered out, and the frequency of each remaining word is calculated across all volumes.\n\nfor (const volume of data.data) {\n        let individualVol = 0;\n          \n        loacalPerVolDict[volume.htid] = {}\n          \n        for (const page of volume.features.pages) {\n          const body = page.body;\n          if (body.tokensCount !== null) {\n              updateDict(body.tokensCount,loacalPerVolDict[volume.htid]);\n          }\n          total += page.tokensCount;\n          individualVol += page.tokensCount;\n        }\n      }\n\nWord Cloud Generation: The most frequent words are selected, and their frequencies are used to determine the size of each word in the word cloud. D3’s Word Cloud component (react-d3-cloud) is used to generate the Word Cloud visualization. This component takes in data in the form of an array of word objects, where each object contains the text of the word and its frequency.\nRendering: The Word Cloud component is rendered within the React component, with additional styling and customization options such as font size, rotation, and padding.\n\n&lt;WordCloud\n      width={1000}\n      height={3000}\n      data={data}\n      fontSize={fontSize}\n      rotate={rotate}\n      padding={6}\n      \n      spiral=\"rectangular\"\n      random={Math.random}      \n    /&gt;\n\nStopwords:\n\n[\n          'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'not', 'in', 'on', 'at', 'to', 'of',\n          'by', 'with', 'about', 'above',  'below', 'over', 'under', 'above', 'below', 'from',\n          'as', 'I', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n          'my', 'your', 'his', 'its', 'our', 'their', 'mine', 'yours', 'hers', 'ours', 'theirs',\n          'this', 'that', 'these', 'those', 'is', 'am', 'are', 'was', 'were', 'be', 'being',\n          'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'shall', 'should', 'would',\n          'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'would', 'cannot', 'could not',\n          'can\\'t', 'couldn\\'t', 'don\\'t', 'didn\\'t', 'won\\'t', 'wouldn\\'t', 'shouldn\\'t', 'mustn\\'t',\n          'let\\'s', 'etc.', '-rrb-', '-lrb-', '--', '-', 'a','b','c','d','e','f','g','h',\n          'i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\"''\",\"``\",\"'s\",\n          \"...\"\n      ];\npunctuationMarks = [\".\", \",\", \"!\", \"?\",\";\",\":\",\"'\"];\nCreating a word cloud often involves filtering out common words, also known as stopwords, to emphasize the more unique or relevant words within a text.\nInteractivity: The component provides interactivity features such as downloading the word cloud as an image (PNG or SVG) or as CSV data.\nSummary: All in all, the Word Cloud widget provides a visually appealing and informative way to explore the most common words in a dataset or text corpus, facilitating quick insights into the underlying textual data.\n\n\n\n\n\n\n\n\n\n\n4.3.2 Pie Chart\nThis pie chart widget, similar to the word cloud widget, serves to visualize data, but in this case, it focuses on the distribution of languages within a given dataset or workset (volumes of novels).\n\n4.3.2.1 Internal working of Word Cloud:\n\nData Fetching: Data is fetched from an external API endpoint using the fetch API. In this case, the API endpoint is https://tools.htrc.illinois.edu/ef-api/worksets/$%7BworksetId%7D/metadata. The fetched data contains information about the languages present in the dataset.\nData Processing: Upon receiving the data, it’s processed to count the occurrences of each language. The count of each language is stored in the languageCount state.\n\nconst count = {};\n      data.data.forEach(item =&gt; {\n        const language = item.metadata.language;\n        if (language) {\n          count[language] = (count[language] || 0) + 1;\n        }\n\nRendering the Pie Chart: The LanguagePieChart component is responsible for rendering the pie chart. It receives the language count data as props. Using D3.js, it generates the pie chart based on this data.\n\n&lt;MainCard\n      content={false}\n      sx={{\n        padding: theme.spacing(8),\n        display: 'flex',\n        flexDirection: 'column',\n        alignItems: 'center',\n        position: 'relative',\n        height: '650px',\n        width: '600px'\n      }}&gt;\n\nDrawing the Chart: Inside the drawChart function, D3.js is used to create the SVG elements necessary for the pie chart. Each language is represented as a slice in the pie chart, with its size proportional to its count in the dataset.\nfunction drawChart() {\n    // Remove the old svg\n    d3.select('#language-pie-container')\n      .select('svg')\n      .remove();\n\n    // Create new svg\n    const svg = d3\n      .select('#language-pie-container')\n      .append('svg')\n      .attr('width', width)\n      .attr('height', height)\n      .append('g')\n      .attr('transform', `translate(${width/2}, ${height/2})`);\n\n\n    const arcGenerator = d3\n    .arc()\n    .innerRadius(innerRadius)\n    .outerRadius(outerRadius);\n\n\n      const pieGenerator = d3\n  .pie&lt;{ label: string; value: number }&gt;() // Explicitly set the type here\n  .padAngle(0)\n  .value((d) =&gt; d.value);\nColor Encoding: D3’s color scale is used to assign colors to each slice of the pie chart. The interpolateCool color scheme is used here.\nUpdating the Chart: The useEffect hook is utilized to update the chart whenever the language count data changes. This ensures that the chart reflects the latest data fetched from the API.\n\nconst formattedData = Object.entries(data).map(([language, count]) =&gt; ({ label: language, value: count }));\nconst arc = svg\n    .selectAll()\n    .data(pieGenerator(formattedData))\n    .enter()\n    .append('path')\n    .attr('d', (d: any) =&gt; arcGenerator(d) as string) // Explicitly cast to string\n    .style('fill', (_, i) =&gt; colorScale(i))\n    .style('stroke', '#ffffff')\n    .style('stroke-width', 0);\nStyling: The pie chart is styled to have a white stroke around each slice and white text labels, providing contrast against the colored slices.\nconst text = svg\n    .selectAll()\n    .data(pieGenerator(formattedData))\n    .enter()\n    .append('text')\n    .attr('text-anchor', 'middle')\n    .attr('alignment-baseline', 'middle')\n    .text((d: any) =&gt; d.data.label)\n    .style('fill', 'white') // Set text color to white\n    //.style('fill', (_, i) =&gt; colorScale(formattedData.length - i))\n    .attr('transform', (d: any) =&gt; {\n      const [x, y] = arcGenerator.centroid(d);\n      return `translate(${x}, ${y})`;\n    });\n}\nSummary: All in all, the pie chart widget provides a visual representation of the distribution of languages within a dataset, enabling users to quickly grasp the relative prevalence of different languages."
  }
]